{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10769964475020082798\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1399655220\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1888547470471192608\n",
      "physical_device_desc: \"device: 0, name: GeForce MX250, pci bus id: 0000:3a:00.0, compute capability: 6.1\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # 초기화할 GPU number\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # GPU 메모리를 전부 할당하지 않고, 아주 적은 비율로 할당되어 시작\n",
    "    # 프로세스의 메모리 수요에 따라 자동적으로 증가\n",
    "    # 단, GPU 메모리를 처음부터 전체 비율로 사용하지 않음\n",
    "    gpu_options = tf.compat.v1.GPUOptions(allow_growth = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'surprise', 'fear', 'sadness', 'joy', 'disgust',\n",
       "       'anger'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "data['Target'].unique() # 타겟 클래스 이름 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             0\n",
       "Utterance      0\n",
       "Speaker        0\n",
       "Dialogue_ID    0\n",
       "Target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum() # 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data['Utterance']\n",
    "y_train = data['Target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9989, 5333)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(x_train)\n",
    "x_train_cnt_vect = cnt_vect.transform(x_train)\n",
    "\n",
    "print(x_train_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liked\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(x_train_cnt_vect, y_train)\n",
    "pred = lr_clf.predict(x_test_cnt_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_submit = pd.read_csv('data/sample_submission.csv')\n",
    "lr_submit['Target'] = pred\n",
    "lr_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_submit.to_csv('./lr_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liked\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(x_train)\n",
    "x_train_tfidf_vect = tfidf_vect.transform(x_train)\n",
    "\n",
    "x_test_tridf_vect = tfidf_vect.transform(x_test)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(x_train_tfidf_vect, y_train)\n",
    "pred = lr_clf.predict(x_test_tridf_vect)\n",
    "lr_submit['Target'] = pred\n",
    "lr_submit.to_csv('./lr_submit_tfidf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tfidf 벡터화 결과 일반 벡터화에 비해 F1 score 0.04점 감소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터 조정 - gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_scaled, val_scaled, train_target, val_target = train_test_split(x_train, y_train, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cnt_vect__max_df': 100, 'cnt_vect__ngram_range': (1, 1), 'lr_clf__C': 0.5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "with tf.device(\"/device:GPU:0\"): # GPU 사용\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "    ('cnt_vect',CountVectorizer(stop_words='english')),\n",
    "    ('lr_clf', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    params = {'cnt_vect__ngram_range' : [(1,1),(1,2),(1,3)],\n",
    "          'cnt_vect__max_df' : [100,300,700],\n",
    "          'lr_clf__C' : [0.5,1,5,10]\n",
    "          }\n",
    "\n",
    "    grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv = 3, scoring = 'f1', verbose = 0)\n",
    "    grid_cv_pipe.fit(train_scaled, train_target)\n",
    "\n",
    "    print(grid_cv_pipe.best_params_)\n",
    "\n",
    "    pred = grid_cv_pipe.predict(val_scaled)\n",
    "    lr_f1 = f1_score(val_target, pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             0\n",
       "Utterance      0\n",
       "Speaker        0\n",
       "Dialogue_ID    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('data/test.csv')\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test['Utterance']\n",
    "pred = grid_cv_pipe.predict(x_test)\n",
    "\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit['Target'] = pred\n",
    "\n",
    "submit.to_csv('submit/lr_param.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43af7393c8e3bb0730eb35f197fc5ab4355397fc73467075a0f020e747dbc619"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
