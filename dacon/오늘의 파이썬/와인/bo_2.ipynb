{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train_one = pd.get_dummies(train)\n",
    "test_one = pd.get_dummies(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_one.drop(columns=['index','quality'])\n",
    "y = train['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_parameter_bounds = {\n",
    "                      'max_depth' : (1,3), # 나무의 깊이\n",
    "                      'n_estimators' : (30,100),\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_bo(max_depth, n_estimators):\n",
    "  rf_params = {\n",
    "              'max_depth' : int(round(max_depth)),\n",
    "               'n_estimators' : int(round(n_estimators)),      \n",
    "              }\n",
    "  rf = RandomForestClassifier(**rf_params)\n",
    "\n",
    "  x_train, x_valid, y_train, y_valid = train_test_split(x,y,test_size = 0.2, )\n",
    "\n",
    "  rf.fit(x_train,y_train)\n",
    "  score = accuracy_score(y_valid, rf.predict(x_valid))\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_rf = BayesianOptimization(f = rf_bo, pbounds = rf_parameter_bounds,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | n_esti... |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.53    \u001b[0m | \u001b[0m 2.098   \u001b[0m | \u001b[0m 80.06   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.52    \u001b[0m | \u001b[0m 2.206   \u001b[0m | \u001b[0m 68.14   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.5436  \u001b[0m | \u001b[95m 1.847   \u001b[0m | \u001b[95m 75.21   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5036  \u001b[0m | \u001b[0m 1.875   \u001b[0m | \u001b[0m 92.42   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5345  \u001b[0m | \u001b[0m 2.927   \u001b[0m | \u001b[0m 56.84   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5255  \u001b[0m | \u001b[0m 2.988   \u001b[0m | \u001b[0m 73.03   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5327  \u001b[0m | \u001b[0m 2.07    \u001b[0m | \u001b[0m 80.1    \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5218  \u001b[0m | \u001b[0m 1.618   \u001b[0m | \u001b[0m 75.33   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.4927  \u001b[0m | \u001b[0m 1.988   \u001b[0m | \u001b[0m 75.16   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5082  \u001b[0m | \u001b[0m 2.036   \u001b[0m | \u001b[0m 80.07   \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "bo_rf.maximize(init_points = 5, n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 1, 'n_estimators': 75}\n"
     ]
    }
   ],
   "source": [
    "max_params = bo_rf.max['params']\n",
    "\n",
    "max_params['max_depth'] = int(max_params['max_depth'])\n",
    "max_params['n_estimators'] = int(max_params['n_estimators'])\n",
    "print(max_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(**max_params)\n",
    "RF.fit(x,y)\n",
    "pred = RF.predict(test_one.drop(columns=['index']))\n",
    "sub = pd.read_csv('submission.csv')\n",
    "sub['quality'] = pred\n",
    "sub.to_csv('tune_rf.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost의 하이퍼 파라미터\n",
    "\n",
    "- learning rate : 높을수록 과적합 되기 쉬움\n",
    "- subsample : weak learner가 학습에 사용하는 데이터 샘플링 비율, 보통 0.5~1 사이이며 낮을수록 과적합 방지\n",
    "- n_estimators : 생성할 weak learner의 수, learning_rate가 낮을 때 n_estimators를 높여야 과적합 방지\n",
    "- colsample_bytree : 각 tree 별 사용된 feature의 퍼센테이지, 보통 0.5~1 사용되며 낮을수록 과적합 방지\n",
    "- max_depth : 트리의 maximum depth, 적절한 값이 제시되어야 하고 보통 3-10 사이, 높을수록 복잡도가 커져 과적합 하기 쉬움\n",
    "- lambda : 가중치에 대한 L2 Regularization 적용 값, 피쳐 개수가 많을 때 적용을 검토하며 값이 클수록 과적합 감소\n",
    "- gamma : 리프노드의 추가분할을 결정할 최소손실 감소값, 해당값보다 손실이 크게 감소할 때 분리\n",
    "- alpha : 가중치에 대한 L1 Regularization 적용 값, 피쳐 개수가 많을 때 적용을 검토하며 값이 클수록 과적합 감소 효과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_parameter_bounds = {\n",
    "                      'gamma' : (0,10),\n",
    "                      'max_depth' : (1,3), # 나무의 깊이\n",
    "                      'subsample' : (0.5,1)\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_bo(gamma,max_depth, subsample):\n",
    "  xgb_params = {\n",
    "              'gamma' : int(round(gamma)),\n",
    "              'max_depth' : int(round(max_depth)),\n",
    "               'subsample' : int(round(subsample)),      \n",
    "              }\n",
    "  xgb = XGBClassifier(**xgb_params)\n",
    "\n",
    "  x_train, x_valid, y_train, y_valid = train_test_split(x,y,test_size = 0.2, )\n",
    "\n",
    "  xgb.fit(x_train,y_train)\n",
    "  score = accuracy_score(y_valid, xgb.predict(x_valid))\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   gamma   | max_depth | subsample |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5664  \u001b[0m | \u001b[0m 5.488   \u001b[0m | \u001b[0m 2.43    \u001b[0m | \u001b[0m 0.8014  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.5664  \u001b[0m | \u001b[0m 5.449   \u001b[0m | \u001b[0m 1.847   \u001b[0m | \u001b[0m 0.8229  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.5809  \u001b[0m | \u001b[95m 4.376   \u001b[0m | \u001b[95m 2.784   \u001b[0m | \u001b[95m 0.9818  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5555  \u001b[0m | \u001b[0m 3.834   \u001b[0m | \u001b[0m 2.583   \u001b[0m | \u001b[0m 0.7644  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5555  \u001b[0m | \u001b[0m 5.68    \u001b[0m | \u001b[0m 2.851   \u001b[0m | \u001b[0m 0.5355  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5245  \u001b[0m | \u001b[0m 6.692   \u001b[0m | \u001b[0m 2.421   \u001b[0m | \u001b[0m 0.9232  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5591  \u001b[0m | \u001b[0m 2.303   \u001b[0m | \u001b[0m 2.915   \u001b[0m | \u001b[0m 0.6916  \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.5836  \u001b[0m | \u001b[95m 4.354   \u001b[0m | \u001b[95m 2.839   \u001b[0m | \u001b[95m 0.9727  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5464  \u001b[0m | \u001b[0m 4.599   \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 0.6215  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5545  \u001b[0m | \u001b[0m 3.72    \u001b[0m | \u001b[0m 1.231   \u001b[0m | \u001b[0m 0.9286  \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "BO_xgb = BayesianOptimization(f = xgb_bo, pbounds = xgb_parameter_bounds,random_state = 0)\n",
    "BO_xgb.maximize(init_points = 5, n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(gamma=4.353687657938132, max_depth=2, objective='multi:softprob',\n",
       "              subsample=0.9726705213125111)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_params = BO_xgb.max['params']\n",
    "max_params['max_depth'] = int(max_params['max_depth'])\n",
    "XGB = XGBClassifier(**max_params)\n",
    "XGB.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = XGB.predict(test_one.drop(columns=['index']))\n",
    "sub = pd.read_csv('submission.csv')\n",
    "sub['quality'] = pred\n",
    "sub.to_csv('tune_xgb.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM 하이퍼 파라미터\n",
    "\n",
    "- learning rate : 높을수록 과적합 되기 쉬움\n",
    "- subsample : weak learner가 학습에 사용하는 데이터 샘플링 비율, 보통 0.5~1 사이이며 낮을수록 과적합 방지\n",
    "- n_estimators : 생성할 weak learner의 수, 너무 크면 과적합 발생\n",
    "- colsample_bytree : 각 tree 별 사용된 feature의 퍼센테이지, 보통 0.5~1 사용되며 낮을수록 과적합 방지\n",
    "- max_depth : 트리의 maximum depth, 적절한 값이 제시되어야 하고 보통 3-10 사이, 기본값은 깊이에 제한이 없음(-1)\n",
    "- reg_lambda : 가중치에 대한 L2 Regularization 적용 값, 피쳐 개수가 많을 때 적용을 검토하며 값이 클수록 과적합 감소\n",
    "- min_child_samples : 최종 리프 노드가 되기 위한 레코드 수로 과적합 제어용\n",
    "- reg_alpha : 가중치에 대한 L1 Regularization 적용 값, 피쳐 개수가 많을 때 적용을 검토하며 값이 클수록 과적합 감소 효과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm_parameter_bounds = {\n",
    "                      'n_estimators' : (30,100),\n",
    "                      'max_depth' : (1,3), \n",
    "                      'subsample' : (0.5,1)\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_bo(n_estimators,max_depth, subsample):\n",
    "\n",
    "  lgbm_params = {\n",
    "              'n_estimators' : int(round(n_estimators)),\n",
    "              'max_depth' : int(round(max_depth)),\n",
    "              'subsample' : int(round(subsample)),      \n",
    "              }\n",
    "\n",
    "  lgbm = LGBMClassifier(**lgbm_params)\n",
    "  \n",
    "  x_train, x_valid, y_train, y_valid = train_test_split(x,y,test_size = 0.2, )\n",
    "\n",
    "  lgbm.fit(x_train,y_train)\n",
    "  score = accuracy_score(y_valid, lgbm.predict(x_valid))\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | n_esti... | subsample |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5736  \u001b[0m | \u001b[0m 2.098   \u001b[0m | \u001b[0m 80.06   \u001b[0m | \u001b[0m 0.8014  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.5627  \u001b[0m | \u001b[0m 2.09    \u001b[0m | \u001b[0m 59.66   \u001b[0m | \u001b[0m 0.8229  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.5745  \u001b[0m | \u001b[95m 1.875   \u001b[0m | \u001b[95m 92.42   \u001b[0m | \u001b[95m 0.9818  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5482  \u001b[0m | \u001b[0m 1.767   \u001b[0m | \u001b[0m 85.42   \u001b[0m | \u001b[0m 0.7644  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5582  \u001b[0m | \u001b[0m 2.136   \u001b[0m | \u001b[0m 94.79   \u001b[0m | \u001b[0m 0.5355  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.56    \u001b[0m | \u001b[0m 2.086   \u001b[0m | \u001b[0m 79.98   \u001b[0m | \u001b[0m 0.7583  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5545  \u001b[0m | \u001b[0m 1.461   \u001b[0m | \u001b[0m 97.03   \u001b[0m | \u001b[0m 0.6916  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5482  \u001b[0m | \u001b[0m 2.082   \u001b[0m | \u001b[0m 68.72   \u001b[0m | \u001b[0m 0.8663  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5264  \u001b[0m | \u001b[0m 1.391   \u001b[0m | \u001b[0m 49.28   \u001b[0m | \u001b[0m 0.5432  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5573  \u001b[0m | \u001b[0m 1.744   \u001b[0m | \u001b[0m 38.08   \u001b[0m | \u001b[0m 0.9286  \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "BO_lgbm = BayesianOptimization(f = lgbm_bo, pbounds = lgbm_parameter_bounds,random_state = 0)\n",
    "BO_lgbm.maximize(init_points = 5, n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(max_depth=1, n_estimators=92, subsample=0.9818313802505146)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_params = BO_lgbm.max['params']\n",
    "max_params['max_depth'] = int(max_params['max_depth'])\n",
    "max_params['n_estimators'] = int(max_params['n_estimators'])\n",
    "LGBM = LGBMClassifier(**max_params)\n",
    "LGBM.fit(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = LGBM.predict(test_one.drop(columns =['index']))\n",
    "sub = pd.read_csv('submission.csv')\n",
    "sub['quality'] = pred\n",
    "sub.to_csv('tune_lgbm.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "VC = VotingClassifier(estimators=[('rf',RF),('xgb',XGB),('lgbm',LGBM)],voting = 'soft')\n",
    "\n",
    "VC.fit(x,y)\n",
    "pred = VC.predict(test_one.drop(columns = ['index']))\n",
    "\n",
    "sub = pd.read_csv('submission.csv')\n",
    "sub['quality'] = pred\n",
    "sub.to_csv('tune_voting.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43af7393c8e3bb0730eb35f197fc5ab4355397fc73467075a0f020e747dbc619"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
