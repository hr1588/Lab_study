input -> function -> output(확률)

문장 -> function -> (긍정, 부정, 중립)일 확률 출력 -> 후처리(긍/부정)

bert, gpt : DL 기반 자연어 처리 model(자연어를 입력받아 해당 입력이 특정 범주일 확률 반환)

학습 -> train data 준비 -> model에 학습(모델이 정답에 가까워지게 update)

트랜스퍼 러닝 : 특정 task를 학습한 모델을 다른 task 수행에 재사용

업스크림 - 프리트레인 / 다운 스크림 - 파인 튜닝

다음 단어 맞히기 -> GPT 계열 모델(언어 모델) / 빈칸 채우기 -> Bert 계열 모델(마스크 언어 모델)

문서 분류 : 자연어 input에 따른 범주(긍정, 중립. 부정)에 속할 확률값 반환

자연어 추론 : 문장 2개를 입력받아 두 문장 사이의 관계가 (참, 거짓, 중립) 등 어떤 범주일지 확률값 반환

개체명 인식 : 자연어를 입력받아 단어별로 기관명, 인명, 지명 등 어떤 개체명 범주에 속하는지 확률값 반환

질의응답 : 자연어(질문+지문)를 입력받아 각 단어가 정답의 시작일 확률값과 끝일 확률값 반환

문장생성 : GPT 계열 언어 모델 사용, 자연어를 입력받아 어휘 전체에 대한 확률값 반환.
여기서 확률값은 입력된 문장 다음에 올 단어로 얼마나 적절한지를 나타내는 점수

 
