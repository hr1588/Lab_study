{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hr1588/deep-learning/blob/main/NLP_2%EC%9E%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58TrNH5OURsA",
        "outputId": "e1a5d32e-49d2-4849-b877-5050d7917b83"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ratsnlp -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWQHAxCNT7OE",
        "outputId": "1d792743-241e-4c0e-f28b-6af0aa393e9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 42 kB 551 kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 8.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 57 kB 3.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 582 kB 53.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 529 kB 46.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 9.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 880 kB 48.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 163 kB 46.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 96 kB 4.6 MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "Zs6rHI7vT9Vi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Caymvsw_OvJp"
      },
      "source": [
        "### NSMC 로드(말뭉치 내려받기 및 전처리)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u0Ss6qVKOvJs",
        "outputId": "c94b31ed-9312-43a3-8cdf-ed216a32fa7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nsmc] download ratings_train.txt: 14.6MB [00:00, 87.9MB/s]                            \n",
            "[nsmc] download ratings_test.txt: 4.90MB [00:00, 32.2MB/s]                           \n"
          ]
        }
      ],
      "source": [
        "from Korpora import Korpora # 패키지 로드\n",
        "nsmc = Korpora.load(\"nsmc\", force_download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FAOyrhNHOvJu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# nsmc 전처리\n",
        "\n",
        "def write_lines(path, lines):\n",
        "    with open(path, 'w', encoding = 'UTF-8') as f:\n",
        "        for line in lines:\n",
        "            f.write(f'{line}\\n')\n",
        "\n",
        "write_lines(\"drive/MyDrive/Bert/train.txt\", nsmc.train.get_all_texts())\n",
        "write_lines(\"drive/MyDrive/Bert/test.txt\", nsmc.test.get_all_texts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NSMC에 포함된 영화 리뷰들을 순수 텍스트 형태로 코랩 환경에 저장"
      ],
      "metadata": {
        "id": "yONbK7adO3nc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBU9Jj2YOvJv"
      },
      "source": [
        "### GPT 토크나이저 구축"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GPT 계열 모델이 사용하는 토크나이저 기법은 BPE\n",
        "- BPE(바이트 페어 인코딩) : 데이터에서 가장 많이 등장한 문자열을 병합해서 데이터를 압축하는 기법, 빈도를 기준으로 병합"
      ],
      "metadata": {
        "id": "m1h5JTitO-pF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cNYoiB7ZOvJv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"bbpe\", exist_ok=True) # 저장 디렉토리 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WhGtRoqWOvJw",
        "outputId": "db5765bf-4727-4a81-cb14-a41bade8f4b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bbpe/vocab.json', 'bbpe/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "bytebpe_tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "bytebpe_tokenizer.train(\n",
        "    files = [\"drive/MyDrive/Bert/train.txt\", \"drive/MyDrive/Bert/test.txt\"], # 학습 말뭉치 리스트 형태로 삽입\n",
        "    vocab_size = 10000, # 어휘 집합 크기 조절\n",
        "    special_tokens = [\"[PAD]\"] # 특수 토큰 추가\n",
        ")\n",
        "\n",
        "bytebpe_tokenizer.save_model(\"bbpe\") "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- json 파일 : 바이트 수준 BPE의 어휘 집합\n",
        "- txt 파일 : 바이그램 쌍의 병합 우선 순위\n",
        "- 바이그램 : 토큰을 2개씩 묶어서 나열한 것\n"
      ],
      "metadata": {
        "id": "lJ1QRlkqPggl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BPE 토큰화 수행 방식\n",
        "\n",
        "1. BPE는 어절별로 병합 우선순위가 높은 바이그램 쌍을 반복해서 병합\n",
        "2. 병합된 토큰이 어휘 집합에 있는지 확인해 최종 결과 도출"
      ],
      "metadata": {
        "id": "SnKRq6HlRqWQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA1ZPoKEOvJw"
      },
      "source": [
        "### Bert 토크나이저 구축"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BERT는 워드피스 토크나이저를 사용\n",
        "- 워드피스(wordpiece) : 말뭉치에서 자주 등장한 문자열을 토큰으로 인식한다는 점이 BPE와 본질적으로 유사하지만, 어휘 집합을 구축할 때 문자열을 병합하는 기준이 다름\n",
        "- 워드패스는 병합했을 때 말뭉치의 우도를 가장 높히는 쌍을 병합\n",
        "- 우도(likelihood) : 고정된 관측값이 어떠한 확률분포에서 어느정도의 확률로 나타나는지에 대한 확률(확률밀도함수가 나타내는 그래프의 y값)"
      ],
      "metadata": {
        "id": "y8Ym71KgQao5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5wQTDLNlOvJw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"wordpiece\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1bFgsaRJOvJx",
        "outputId": "350aefcf-e670-4fdd-c04b-ba51d3468848",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wordpiece/vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "wordpiece_tokenizer = BertWordPieceTokenizer(lowercase = False)\n",
        "wordpiece_tokenizer.train(\n",
        "    files = [\"drive/MyDrive/Bert/train.txt\",\"drive/MyDrive/Bert/test.txt\"],\n",
        "    vocab_size = 10000\n",
        ")\n",
        "\n",
        "wordpiece_tokenizer.save_model(\"wordpiece\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "워드피스 토큰화 수행 방식\n",
        "\n",
        "1. 분석 대상 어절에 어휘 집합에 있는 서브워드가 포함되어 있을 때 해당 서브워드를 어절에서 분리\n",
        "\n",
        "2. 어절의 나머지에서 어휘 집합에 있는 서브워드 탐색(최장 일치 기준) 후 분리\n",
        "\n",
        "- 이 때, 분석 대상 문자열에서 서브워드 후보가 하나도 없으면 해당 문자열 전체를 미등록 단어 취급"
      ],
      "metadata": {
        "id": "hOA24jcMRx_w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_isqDrHGOvJx"
      },
      "source": [
        "### GPT 입력값 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BNKI0DGuOvJy",
        "outputId": "acc1dd3f-9f8f-4f6c-b6ea-10ed8d7462b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "file bbpe/config.json not found\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer_gpt = GPT2Tokenizer.from_pretrained(\"bbpe\")\n",
        "tokenizer_gpt.pad_token = \"[PAD]\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 입력값을 만들기 위해 BPE 어휘 집합과 바이그램 병합 우선순위 필요"
      ],
      "metadata": {
        "id": "DZSlcbmWSe7K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2OwgrM1gOvJy",
        "outputId": "b9bd4d27-42a0-4cf2-cc04-d09788e9f6ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['ìķĦ', 'ĠëįĶë¹Ļ', '..', 'Ġì§Ħì§ľ', 'Ġì§ľì¦ĿëĤĺ', 'ëĦ¤ìļĶ', 'Ġëª©ìĨĮë¦¬'], ['íĿł', '..', 'Ġíı¬ìĬ¤íĦ°', 'ë³´ê³ł', 'Ġì´ĪëĶ©', 'ìĺģíĻĶ', 'ì¤Ħ', ',,,', 'Ġìĺ¤ë²Ħ', 'ìĹ°ê¸°', 'ì¡°ì°¨', 'Ġê°Ģë³į', 'ì§Ģ', 'ĠìķĬ', 'êµ¬ëĤĺ'], ['ìĭ¬', 'ê¸Ī', 'ìĿĦ', 'Ġìļ¸ë¦¬ëĬĶ', 'ĠìĺģíĻĶëĭ¤'], ['ëĳĺ', 'ìĿ´', 'ìĦľ', 'Ġë³´ëĭ¤ê°Ģ', 'ĠíķĺëĤĺê°Ģ', 'Ġì£½', 'ìĸ´ëıĦ', 'Ġëª¨ë¥´ê²łëĭ¤'], ['ìĿ´ìłľ', 'ê»ı', 'ĠìĿ´ëŁ°', 'ĠìĦ±', 'íĸ¥', 'ìĿĺ', 'ĠìĺģíĻĶë¥¼', 'Ġë³¸', 'Ġìłģ', 'ìĿ´', 'ĠìĹĨëĭ¤', 'Ġ']]\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "    \"흠.. 포스터보고 초딩영화줄,,, 오버연기조차 가볍지 않구나\",\n",
        "    \"심금을 울리는 영화다\",\n",
        "    \"둘이서 보다가 하나가 죽어도 모르겠다\",\n",
        "    \"이제껏 이런 성향의 영화를 본 적이 없다 \"\n",
        "]\n",
        "\n",
        "tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]\n",
        "print(tokenized_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kzFMt8mdOvJz"
      },
      "outputs": [],
      "source": [
        "batch_input = tokenizer_gpt(\n",
        "    sentences,\n",
        "    padding = \"max_length\", # 문장의 최대 길이에 맞춰 패딩\n",
        "    max_length = 12,        # 문장의 토큰 기준 최대 길이\n",
        "    truncation = True       # 문장 잘림 허용 여부\n",
        ")\n",
        "\n",
        "# 코드의 실행 결과로 input_ids, attention_mask 입력값 제작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dx6VZK-6OvJz",
        "outputId": "92c86615-f8ac-4b56-a5f7-3e930d45972d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[334, 2338, 263, 581, 4055, 464, 3808, 0, 0, 0, 0, 0],\n",
              " [3693, 263, 2720, 758, 2883, 356, 806, 1907, 4444, 875, 2960, 7292],\n",
              " [669, 616, 313, 6992, 1200, 0, 0, 0, 0, 0, 0, 0],\n",
              " [3518, 264, 318, 1876, 9347, 916, 1553, 2132, 0, 0, 0, 0],\n",
              " [2310, 3289, 649, 963, 1588, 306, 777, 626, 1284, 264, 897, 221]]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "batch_input[\"input_ids\"] "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토큰화 결과를 가지고 각 토큰을 인덱스로 바꾼 것, 어휘 집합을 통해 확인할 수 있음\n",
        "- 인덱싱 : 각 토큰을 인덱스로 변환하는 과정"
      ],
      "metadata": {
        "id": "1bkD54geSyzj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ABg0UOtVOvJ0",
        "outputId": "90efce74-ea94-4bb6-d292-8c36c1aa7039",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "batch_input[\"attention_mask\"] # attention_mask : 일반 토큰이 자리한 곳과 패딩 토큰이 자리한 곳을 구분해 알려주는 장치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIOND48ZOvJ0"
      },
      "source": [
        "### Bert 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Dd42ZJdiOvJ0",
        "outputId": "f16d326d-6e6b-4e8c-d398-013ea8fb30f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "file wordpiece/config.json not found\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer_bert = BertTokenizer.from_pretrained(\n",
        "    \"wordpiece\", do_lower_case = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "H2Oi9WY3OvJ0"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences_bert = [tokenizer_bert.tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "batch_inputs = tokenizer_bert(\n",
        "    sentences,\n",
        "    padding = \"max_length\",\n",
        "    max_length = 12,\n",
        "    truncation = True\n",
        ")\n",
        "\n",
        "# 코드의 실행결과로 3가지 입력값 제작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qHFmaffoOvJ1",
        "outputId": "13da01fa-504c-4ef5-e919-e8ef7fc77085",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 621, 2631, 16, 16, 1993, 3678, 1990, 3323, 3, 0, 0],\n",
              " [2, 997, 16, 16, 2609, 2045, 2796, 1981, 1115, 14, 14, 3],\n",
              " [2, 601, 1087, 1031, 6440, 2195, 3, 0, 0, 0, 0, 0],\n",
              " [2, 5014, 1105, 2382, 8357, 8422, 1037, 2693, 3, 0, 0, 0],\n",
              " [2, 2233, 1624, 1999, 564, 1317, 1044, 2038, 503, 9035, 2080, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "batch_inputs[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV8kbCJHOvJ1"
      },
      "source": [
        "- CLS : 모든 문장 앞에 대응하는 인덱스(2)\n",
        "- SEP : 모든 문장 뒤에 대응하는 인덱스(3)\n",
        "- Bert는 문장 시작과 끝에 이 2개 토큰을 덧붙임"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yuGYp7LFOvJ1",
        "outputId": "084c01cb-9912-470d-9a1b-ce1131fe35c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "batch_inputs['attention_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-RUxY7yoOvJ2",
        "outputId": "374adf26-2bd5-4951-e97c-902caeff75c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "batch_inputs[\"token_type_ids\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- token_type_ids : 세그먼트(segment)에 해당하는 입력값\n",
        "- BERT 모델은 기본적으로 문서(문장) 2개를 입력받는데, 이 때 token_type_ids로 구분\n",
        "- 첫 번째 세그먼트에 해당하는 입력값은 0, 두 번째 세그먼트는 1\n",
        "- 해당 코드에서는 문장을 하나씩 넣었기 때문에 모든 값이 0"
      ],
      "metadata": {
        "id": "j65ntsYqTeYt"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "43af7393c8e3bb0730eb35f197fc5ab4355397fc73467075a0f020e747dbc619"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
